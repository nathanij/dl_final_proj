{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3dun9PuWne1"
      },
      "source": [
        "Use cityscapes dataset, trains directly on damaged images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkgOXuCYeqdh"
      },
      "source": [
        "Beginning with U-net architecture, will explore other as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BTt2OmoWrzX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "from glob import glob\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transform\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/Users/nathanieljames/Desktop/dl_final_proj/dl_final_proj\")\n",
        "from utils.cityscapes import CityscapesDataset, get_loaders\n",
        "\n",
        "#dataset: https://www.kaggle.com/datasets/dansbecker/cityscapes-image-pairs/data\n",
        "# change to germany se"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GY-amT9CIt7L",
        "outputId": "72d8c95b-f9cc-43c3-9355-0606f54634b8"
      },
      "outputs": [],
      "source": [
        "dataloader, valloader = get_loaders(batch_size=32, subclass = \"5\")\n",
        "print(len(dataloader), len(valloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxrb_v9EhpgJ",
        "outputId": "0f7eaf66-1e5e-4a8e-c328-c22aab7ba076"
      },
      "outputs": [],
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"mps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkxAHz4yRxkF"
      },
      "outputs": [],
      "source": [
        "class Convblock(nn.Module):\n",
        "\n",
        "      def __init__(self,input_channel,output_channel,kernel=3,stride=1,padding=2):\n",
        "\n",
        "        super().__init__()\n",
        "        self.convblock = nn.Sequential(\n",
        "            nn.Conv2d(input_channel,output_channel,kernel,stride,padding),\n",
        "            nn.BatchNorm2d(output_channel),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(output_channel,output_channel,kernel),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "      def forward(self,x):\n",
        "        x = self.convblock(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A-0afzjRyVy"
      },
      "outputs": [],
      "source": [
        "class DirectUNet(nn.Module):\n",
        "\n",
        "    def __init__(self,input_channel,retain=True):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = Convblock(input_channel,32)\n",
        "        self.conv2 = Convblock(32,64)\n",
        "        self.conv3 = Convblock(64,128, kernel = 1, padding = 0)\n",
        "        # next 4 lines are bottom layer\n",
        "        self.conv4 = Convblock(128,256, kernel = 1, padding = 0)\n",
        "        self.neck = nn.Conv2d(256,512,3,1)\n",
        "        self.upconv4 = nn.ConvTranspose2d(512,256,3,2,0,1)\n",
        "        self.dconv4 = Convblock(512,256)\n",
        "        # begin replace here\n",
        "        #self.neck = nn.Conv2d(256, 256, 3, 1)\n",
        "        self.upconv3 = nn.ConvTranspose2d(256,128,3,2,0,1)\n",
        "        self.dconv3 = Convblock(256,128)\n",
        "        self.upconv2 = nn.ConvTranspose2d(128,64,3,2,0,1)\n",
        "        self.dconv2 = Convblock(128,64)\n",
        "        self.upconv1 = nn.ConvTranspose2d(64,32,3,2,0,1)\n",
        "        self.dconv1 = Convblock(64,32)\n",
        "        self.out = nn.Conv2d(32,3,1,1,1)\n",
        "        self.retain = retain\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        # Encoder Network\n",
        "\n",
        "        # Conv down 1\n",
        "        conv1 = self.conv1(x)\n",
        "        pool1 = F.max_pool2d(conv1,kernel_size=2,stride=2)\n",
        "        # Conv down 2\n",
        "        conv2 = self.conv2(pool1)\n",
        "        pool2 = F.max_pool2d(conv2,kernel_size=2,stride=2)\n",
        "        # Conv down 3\n",
        "        conv3 = self.conv3(pool2)\n",
        "        pool3 = F.max_pool2d(conv3,kernel_size=2,stride=2)\n",
        "        # Conv down 4\n",
        "        conv4 = self.conv4(pool3)\n",
        "        pool4 = F.max_pool2d(conv4,kernel_size=2,stride=2)\n",
        "\n",
        "        # BottelNeck\n",
        "        neck = self.neck(pool4) #cb to pool 4 if layer 4 left in\n",
        "\n",
        "        # Decoder Network\n",
        "\n",
        "        # Upconv 1 again removed\n",
        "        upconv4 = self.upconv4(neck)\n",
        "        croped = self.crop(conv4,upconv4)\n",
        "        # # Making the skip connection 1\n",
        "        dconv4 = self.dconv4(torch.cat([upconv4,croped],1))\n",
        "        # Upconv 2\n",
        "        upconv3 = self.upconv3(dconv4) # replace to dconv 4 if unskip\n",
        "        croped = self.crop(conv3,upconv3)\n",
        "        # Making the skip connection 2\n",
        "        dconv3 = self.dconv3(torch.cat([upconv3,croped],1))\n",
        "        # Upconv 3\n",
        "        upconv2 = self.upconv2(dconv3)\n",
        "        croped = self.crop(conv2,upconv2)\n",
        "        # Making the skip connection 3\n",
        "        dconv2 = self.dconv2(torch.cat([upconv2,croped],1))\n",
        "        # Upconv 4\n",
        "        upconv1 = self.upconv1(dconv2)\n",
        "        croped = self.crop(conv1,upconv1)\n",
        "        # Making the skip connection 4\n",
        "        dconv1 = self.dconv1(torch.cat([upconv1,croped],1))\n",
        "        # Output Layer\n",
        "        out = self.out(dconv1)\n",
        "\n",
        "        if self.retain == True:\n",
        "            out = F.interpolate(out,list(x.shape)[2:])\n",
        "\n",
        "        return out\n",
        "\n",
        "    def crop(self,input_tensor,target_tensor):\n",
        "        # Crops for border kernels\n",
        "        _,_,H,W = target_tensor.shape\n",
        "        return transform.CenterCrop([H,W])(input_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrhstbVHR3gN",
        "outputId": "f2ad9ba1-1b11-411c-bf3c-4784025a15d5"
      },
      "outputs": [],
      "source": [
        "model = DirectUNet(3).float()\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(model, (3,256,256))\n",
        "model = model.to(device)\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "loss_func = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.015)\n",
        "\n",
        "train_acc = []\n",
        "val_acc = []\n",
        "train_loss = []\n",
        "val_loss = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "G1qMztyhR69Z",
        "outputId": "41eb80d8-e130-468a-8297-2d919f62a0e6"
      },
      "outputs": [],
      "source": [
        "best_loss = float('inf')\n",
        "for i in range(epochs):\n",
        "\n",
        "    trainloss = 0\n",
        "    valloss = 0\n",
        "\n",
        "    for img,label in tqdm(dataloader):\n",
        "        #print(\"new image\")\n",
        "        optimizer.zero_grad()\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(img)\n",
        "        loss = loss_func(output,label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        trainloss+=loss.item()\n",
        "\n",
        "    train_loss.append(trainloss/len(dataloader))\n",
        "\n",
        "    for img,label in tqdm(valloader):\n",
        "        img = img.to(device)\n",
        "        label = label.to(device)\n",
        "        output = model(img)\n",
        "        loss = loss_func(output,label)\n",
        "        valloss+=loss.item()\n",
        "    epoch_loss = valloss/len(valloader)\n",
        "    val_loss.append(valloss/len(valloader))\n",
        "    print(\"epoch : {} ,train loss : {} ,valid loss : {} \".format(i,train_loss[-1],val_loss[-1]))\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        print(f\"new best loss: {best_loss}\")\n",
        "        print(\"saving\")\n",
        "        torch.save(model.state_dict(), '/content/drive/MyDrive/direct_unet_2.pth')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
