{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from glob import glob\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transform\n",
    "from torch.utils.data import DataLoader,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CityscapesDataset(Dataset):\n",
    "    def __init__(self, image_dir, cut_half = True, transform = None):\n",
    "        self.image_dir = image_dir\n",
    "        self.imgs = os.listdir(image_dir)\n",
    "\n",
    "        self.cut_half = cut_half\n",
    "        self.transforms = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_mask = Image.open(os.path.join(self.image_dir, self.imgs[idx]))\n",
    "        if self.cut_half:\n",
    "            x_width, y_height = img_mask.size\n",
    "            split = x_width / 2\n",
    "\n",
    "            img = img_mask.crop((0, 0, split, y_height))\n",
    "\n",
    "            mask = img_mask.crop((split, 0, split + split, y_height))\n",
    "\n",
    "            if self.transforms:\n",
    "                img = self.transforms(img)\n",
    "                mask = self.transforms(mask)\n",
    "\n",
    "            return img, mask\n",
    "\n",
    "        return img_mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
